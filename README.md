# Ollama Chat GUI

Made by Claude and unsure if other Ollama versions work but "gemma3:1b" works.

## Overview

A modern, user-friendly graphical interface for interacting with Ollama language models. This application provides a clean chat interface to communicate with locally-running Ollama models.

## Features

- Clean, modern interface with message bubbles for user and AI messages
- Automatic model detection from your Ollama installation
- Real-time streaming responses with typing indicator
- Conversation history within the session
- Simple and intuitive design

## Requirements

- Python 3.x
- Tkinter (usually comes with Python)
- Requests library (`pip install requests`)
- Ollama running locally on the default port (11434)

## Installation

1. Make sure you have Ollama installed and running: [Ollama Installation Guide](https://github.com/ollama/ollama)
2. Install the required Python package: `pip install requests`
3. Download the script and run it: `python ollama_gui.py`

## Usage

1. Launch the application
2. Select a model from the dropdown menu
3. Type your message and press Enter or click Send
4. The AI will respond with a streaming reply

The application will automatically detect available models from your Ollama installation.

## Note

This is a simple desktop client for Ollama that requires the Ollama service to be running locally.

([https://media.giphy.com/media/vFKqnCdLPNOKc/giphy.gif](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExZWFobDF1cGs2YWV3bDY3MTlkNHF3aXVmZGVwczMyZGRxaGRyd20waiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/jiUIQb66dVGsVAZhdA/giphy.gif))
this took me 20 min to somehow show it (dont bully me i'll cum :(
